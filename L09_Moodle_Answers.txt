L09 Neural Networks - Moodle Answers

Task 1a.1: Output of the network for x₁=1 and x₂=1
Answer: 6

Task 1a.2: Output of the network for x₁=1 and x₂=-1
Answer: 0

Task 2a: Forward pass implementation
def forward_pass(x, W, u):
    hidden_outputs = activation(x.T @ W)
    y = activation(u @ hidden_outputs)
    return y

Task 2b: Weight updates in stochastic gradient descent
W = W - learning_rate * grad_w
u = u - learning_rate * grad_u

Also: only works if losses are computed in the following way:
# compute and log squared error for the current sample
#loss = 0.5 * (y_t - forward_pass(x_t, W, u))**2
full_loss = 0
for i in range(4):
    full_loss += 0.5 * (y[i] - forward_pass(X[i], W, u))**2
full_loss /= 4
losses.append(full_loss)

Graphic found in repo :)

Task 3a: Option with biggest impact on model performance
- Adding bias terms (use_bias=True)
-> Final accuracy achieved: 1.0 (100%)